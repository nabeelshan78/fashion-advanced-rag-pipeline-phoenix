{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1dedca-9c1c-4af4-86ba-dc61934feab7",
   "metadata": {},
   "source": [
    "## Telemetry Using Phoenix\n",
    "\n",
    "Phoenix is a powerful tool designed to simplify the management and visualization of telemetry data. It helps us handle complex traces, making it easier to analyze and diagnose issues in our system. With Phoenix, we can monitor our RAG system's performance, identify bottlenecks, and gain insights into how different components of our application interact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d54884ff-462f-4ac5-b548-9ea794eb5663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import phoenix as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b041b4f-7f32-4685-81d8-05cc1c4d8c53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a56f570-0335-46d8-86b2-63c6bd9993dd",
   "metadata": {},
   "source": [
    "### Launching Phoenix App\n",
    "\n",
    "Next cell to launch the Phoenix app, which will set up a local server and host a user interface (UI). The default URL for accessing the app is `localhost:6006`, and this will be displayed once we call the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb030cfd-bbc7-48ba-b4ed-ef30857e5768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFOLLOW THIS URL TO OPEN THE UI: http://widxvlspbcig.labs.coursera.org\u001b[0m\n",
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x7e4068df6aa0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.make_url()\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ef342-1cd6-4537-9b8d-ed2de9b8e8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5661d236-40ba-4f09-b007-fa3dd2bbf097",
   "metadata": {},
   "source": [
    "### Preparing the telemetry\n",
    "\n",
    "Now we'll configure the telemetry to work with Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7343f2a-2573-4dc1-8594-d7e5b3a8c685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: example-rag-pipeline\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: http://127.0.0.1:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "phoenix_project_name = \"example-rag-pipeline\"\n",
    "\n",
    "# With phoenix, we just need to register to get the tracer provider with the appropriate endpoint.\n",
    "endpoint=\"http://127.0.0.1:6006/v1/traces\"\n",
    "tracer_provider_phoenix = register(project_name=phoenix_project_name, endpoint = endpoint)\n",
    "\n",
    "# Retrieve a tracer for manual instrumentation\n",
    "tracer = tracer_provider_phoenix.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e0263-7886-4f4c-97bb-ebe70bc824a1",
   "metadata": {},
   "source": [
    "### Using the Pipeline\n",
    "\n",
    "#### Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "663db7ed-3701-4f23-aeb5-f1064a5b6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, fail=False):\n",
    "    # Start a span to trace the retrieval process. Now we can pass a span kind: retriever\n",
    "    with tracer.start_as_current_span(\"retrieving_documents\", openinference_span_kind = 'retriever') as span:\n",
    "        # Log the event of starting retrieval\n",
    "        span.add_event(\"Starting retrieve\")\n",
    "        # Record the input query as an attribute for visibility\n",
    "        # Phoenix allows us to use span.set_input\n",
    "        span.set_input(query)\n",
    "        try:\n",
    "            # Simulate a retrieval failure if 'fail' is True\n",
    "            if fail:\n",
    "                raise ValueError(f\"Retrieve failed for query: {query}\")\n",
    "\n",
    "            # Simulated list of retrieved documents\n",
    "            retrieved_docs = ['retrieved doc1', 'retrieved doc2', 'retrieved doc3']\n",
    "            # Record details about each retrieved document\n",
    "            for i, doc in enumerate(retrieved_docs):\n",
    "                span.set_attribute(f\"retrieval.documents.{i}.document.id\", i)\n",
    "                span.set_attribute(f\"retrieval.documents.{i}.document.content\", doc)\n",
    "                span.set_attribute(f\"retrieval.documents.{i}.document.metadata\", f\"Metadata for document {i}\")\n",
    "        except Exception as e:\n",
    "            # If an exception occurs, log and set the span status to indicate an error\n",
    "            span.set_status(Status(StatusCode.ERROR, str(e)))\n",
    "            span.set_attribute(\"error.type\", type(e).__name__)\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "            # Reraise the exception for handling by the caller\n",
    "            raise\n",
    "\n",
    "        # Mark the span as successful if no error was raised\n",
    "        span.set_status(Status(StatusCode.OK))\n",
    "        return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516b1818-1a8d-4a37-bd53-71043889091c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebb0b0a7-e150-451e-94a1-323936dd453d",
   "metadata": {},
   "source": [
    "### Chains\n",
    "\n",
    "A chain is a connection point between different steps in an LLM application. It links together various operations, like starting a request or passing information from a retriever to an LLM call. Chains help keep things organized and simple. \n",
    "\n",
    "#### The remaining RAG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc849e0-9339-4943-ae03-a8b960896bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def format_documents(retrieved_docs):\n",
    "    t = ''\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        t += f'Retrieved doc: {doc}\\n'\n",
    "    return t\n",
    "\n",
    "@tracer.chain\n",
    "def augment_prompt(query, formatted_documents):\n",
    "    # Create a prompt that combines the query and formatted documents\n",
    "    PROMPT = f\"Answer the query: {query}.\\nRelevant documents:\\n{formatted_documents}\"\n",
    "    return PROMPT\n",
    "\n",
    "@tracer.chain\n",
    "def generate(prompt):\n",
    "    generated_text = f\"Generated text for prompt {prompt}\"\n",
    "    return generated_text\n",
    "\n",
    "@tracer.chain\n",
    "def rag_pipeline(query, fail = False):\n",
    "        # Step 1: Retrieve documents based on the query\n",
    "        retrieved_docs = retrieve(query, fail = fail)\n",
    "        # Step 2: Format the retrieved documents\n",
    "        formatted_docs = format_documents(retrieved_docs)\n",
    "        # Step 3: Augment the query with relevant documents to form a prompt\n",
    "        prompt = augment_prompt(query, formatted_docs)\n",
    "        # Step 4: Generate a response from the augmented prompt\n",
    "        generated_response = generate(prompt)\n",
    "        return generated_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00882f5-0356-4c5c-9886-d74f77e0574b",
   "metadata": {},
   "source": [
    "### Using the UI to analyze the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1574590-634b-4d77-aec1-5213412cd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rag_pipeline(\"This is a test query\")\n",
    "try:\n",
    "    response = rag_pipeline(\"This is a test query that failed\", fail = True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acb7fcf3-db26-477f-b040-3b3e4c7ae918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFOLLOW THIS URL TO OPEN THE UI: http://widxvlspbcig.labs.coursera.org\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "utils.make_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bc7b70-e97e-46b2-944c-a5adbea1662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.restart_kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86da67b1-08a1-40a2-a19f-cc5737135d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b32022cc-e76c-4103-b817-fda9b3b7d2c9",
   "metadata": {},
   "source": [
    "## Tracing and Evaluation with Weaviate\n",
    "\n",
    "Implement a small RAG pipeline to answer an FAQ related question for a clothing store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d0b04e-b13d-429c-8a14-04e018c353fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'flask_app'\n",
      " * Debug mode: off\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "import phoenix as px\n",
    "import flask_app\n",
    "import weaviate\n",
    "import utils\n",
    "import weaviate_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd7f1fb5-73cc-4827-8b3c-7036d4b70235",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFOLLOW THIS URL TO OPEN THE UI: http://widxvlspbcig.labs.coursera.org\u001b[0m\n",
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üìñ For more information on how to use Phoenix, check out https://arize.com/docs/phoenix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown project: UHJvamVjdDoy\n",
      "\n",
      "GraphQL request:4:3\n",
      "3 | ) {\n",
      "4 |   node(id: $id) {\n",
      "  |   ^\n",
      "5 |     __typename\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/graphql/execution/execute.py\", line 530, in await_result\n",
      "    return_type, field_nodes, info, path, await result\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/schema/schema_converter.py\", line 788, in _async_resolver\n",
      "    return await await_maybe(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/utils/await_maybe.py\", line 13, in await_maybe\n",
      "    return await value\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/phoenix/server/api/queries.py\", line 530, in node\n",
      "    raise NotFound(f\"Unknown project: {id}\")\n",
      "phoenix.server.api.exceptions.NotFound: Unknown project: UHJvamVjdDoy\n",
      "Unknown project: UHJvamVjdDoy\n",
      "\n",
      "GraphQL request:4:3\n",
      "3 | ) {\n",
      "4 |   node(id: $id) {\n",
      "  |   ^\n",
      "5 |     __typename\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/graphql/execution/execute.py\", line 530, in await_result\n",
      "    return_type, field_nodes, info, path, await result\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/schema/schema_converter.py\", line 788, in _async_resolver\n",
      "    return await await_maybe(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/utils/await_maybe.py\", line 13, in await_maybe\n",
      "    return await value\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/phoenix/server/api/queries.py\", line 530, in node\n",
      "    raise NotFound(f\"Unknown project: {id}\")\n",
      "phoenix.server.api.exceptions.NotFound: Unknown project: UHJvamVjdDoy\n",
      "Unknown project: UHJvamVjdDoy\n",
      "\n",
      "GraphQL request:4:3\n",
      "3 | ) {\n",
      "4 |   node(id: $id) {\n",
      "  |   ^\n",
      "5 |     __typename\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/graphql/execution/execute.py\", line 530, in await_result\n",
      "    return_type, field_nodes, info, path, await result\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/schema/schema_converter.py\", line 788, in _async_resolver\n",
      "    return await await_maybe(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/utils/await_maybe.py\", line 13, in await_maybe\n",
      "    return await value\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/phoenix/server/api/queries.py\", line 530, in node\n",
      "    raise NotFound(f\"Unknown project: {id}\")\n",
      "phoenix.server.api.exceptions.NotFound: Unknown project: UHJvamVjdDoy\n",
      "Unknown project: UHJvamVjdDoy\n",
      "\n",
      "GraphQL request:4:3\n",
      "3 | ) {\n",
      "4 |   project: node(id: $id) {\n",
      "  |   ^\n",
      "5 |     __typename\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/graphql/execution/execute.py\", line 530, in await_result\n",
      "    return_type, field_nodes, info, path, await result\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/schema/schema_converter.py\", line 788, in _async_resolver\n",
      "    return await await_maybe(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/utils/await_maybe.py\", line 13, in await_maybe\n",
      "    return await value\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/phoenix/server/api/queries.py\", line 530, in node\n",
      "    raise NotFound(f\"Unknown project: {id}\")\n",
      "phoenix.server.api.exceptions.NotFound: Unknown project: UHJvamVjdDoy\n",
      "Unknown project: UHJvamVjdDoy\n",
      "\n",
      "GraphQL request:4:3\n",
      "3 | ) {\n",
      "4 |   project: node(id: $id) {\n",
      "  |   ^\n",
      "5 |     __typename\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/graphql/execution/execute.py\", line 530, in await_result\n",
      "    return_type, field_nodes, info, path, await result\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/schema/schema_converter.py\", line 788, in _async_resolver\n",
      "    return await await_maybe(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/utils/await_maybe.py\", line 13, in await_maybe\n",
      "    return await value\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/phoenix/server/api/queries.py\", line 530, in node\n",
      "    raise NotFound(f\"Unknown project: {id}\")\n",
      "phoenix.server.api.exceptions.NotFound: Unknown project: UHJvamVjdDoy\n",
      "Unknown project: UHJvamVjdDoy\n",
      "\n",
      "GraphQL request:4:3\n",
      "3 | ) {\n",
      "4 |   project: node(id: $id) {\n",
      "  |   ^\n",
      "5 |     __typename\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/graphql/execution/execute.py\", line 530, in await_result\n",
      "    return_type, field_nodes, info, path, await result\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/schema/schema_converter.py\", line 788, in _async_resolver\n",
      "    return await await_maybe(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/strawberry/utils/await_maybe.py\", line 13, in await_maybe\n",
      "    return await value\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/phoenix/server/api/queries.py\", line 530, in node\n",
      "    raise NotFound(f\"Unknown project: {id}\")\n",
      "phoenix.server.api.exceptions.NotFound: Unknown project: UHJvamVjdDoy\n"
     ]
    }
   ],
   "source": [
    "utils.make_url()\n",
    "session = px.launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a24e48-1331-43bc-85c3-adeb6b45521f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12b40798-2c5e-495f-8d63-53cb1f362303",
   "metadata": {},
   "source": [
    "### Configuring the tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a029c7-7384-496c-a9b7-6c58559e3da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: rag-pipeline-with-weaviate\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: http://127.0.0.1:6006/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "phoenix_project_name = \"rag-pipeline-with-weaviate\"\n",
    "\n",
    "# With phoenix, we just need to register to get the tracer provider with the appropriate endpoint. Providing auto_instrument = True, OpenAI calls are automatically traced\n",
    "# TogetherAI is OpenAI compatible!\n",
    "tracer_provider_phoenix = register(project_name=phoenix_project_name, endpoint=\"http://127.0.0.1:6006/v1/traces\", auto_instrument=True)\n",
    "\n",
    "# Retrieve a tracer for manual instrumentation\n",
    "tracer = tracer_provider_phoenix.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f916f3-a511-4c75-9df8-d43f931f9642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f344cff-cf1e-4caa-9000-56da32e4c448",
   "metadata": {},
   "source": [
    "### Preparing the Weaviate client and collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c020e36-f6dd-4cbc-8685-dda7c236ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting the weaviate client\n",
    "client = weaviate.connect_to_local(port=8079, grpc_port=50050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b14719-536d-42cf-83fb-d28ae75132a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "data = joblib.load(\"faq.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c377e4-3729-46e3-ac44-eb8a5ece250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What are your store hours?',\n",
       " 'answer': 'Our online store is open 24/7. Customer service is available from 9:00 AM to 6:00 PM, Monday through Friday.',\n",
       " 'type': 'general information'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dcc8bd-12a1-435a-b8b6-4398fab922b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the collection\n",
    "collection = client.collections.get(\"Faq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41f4580a-878c-4224-b96a-a01e4bacb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d65f111-e617-40a1-8b5d-c0770c482ed2",
   "metadata": {},
   "source": [
    "### The Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c49aca49-bc51-48d7-87de-412c156178cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query_text, limit=5):\n",
    "    # Start a span for the query\n",
    "    with tracer.start_as_current_span(\n",
    "        \"query_weaviate\", openinference_span_kind=\"retriever\"\n",
    "    ) as span:\n",
    "        # Set the input for the span\n",
    "        span.set_input(query_text)\n",
    "\n",
    "        # Query the collection\n",
    "        collection_name = \"Faq\"\n",
    "        chunks = client.collections.get(collection_name)\n",
    "        results = chunks.query.near_text(query=query_text, limit=limit)\n",
    "\n",
    "        # Set the retrieved documents as attributes on the span\n",
    "        for i, document in enumerate(results.objects):\n",
    "            span.set_attribute(f\"retrieval.documents.{i}.document.id\", str(document.uuid))\n",
    "            span.set_attribute(f\"retrieval.documents.{i}.document.metadata\", str(document.metadata))\n",
    "            span.set_attribute(\n",
    "                f\"retrieval.documents.{i}.document.content\", str(document.properties)\n",
    "            )  \n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a30692c-71b6-45f5-aca6-0d9bbd7238ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and format the retrieved results\n",
    "@tracer.chain \n",
    "def format_context(results):\n",
    "    context = \"\"\n",
    "    for item in results.objects:\n",
    "        properties = item.properties\n",
    "        context += f\"Question: {properties['question']}\\n\"\n",
    "        context += f\"Answer: {properties['answer']}\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2dc9993-ec82-41c6-bc29-78344a7fb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt with the retrieved information\n",
    "@tracer.chain\n",
    "def create_prompt(query_text, context):\n",
    "    prompt = f\"\"\"\n",
    "Based on the following information, please answer the FAQ related question: \"{query_text}\"\n",
    "\n",
    "Relevant FAQ (ordered by relevance):\n",
    "{context}\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab23f6b4-43d9-4fea-89d4-af5ff2dc12dc",
   "metadata": {},
   "source": [
    "### LLM call with `openai` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "330e60da-2ec0-4116-9227-0e17475de088",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from openai import OpenAI, DefaultHttpxClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "027eb1df-eeb4-4aee-85cd-c9629c3bd43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transport to bypass SSL verification\n",
    "transport = httpx.HTTPTransport(local_address=\"0.0.0.0\", verify=False)\n",
    "\n",
    "# Create a DefaultHttpxClient instance with the custom transport\n",
    "http_client = DefaultHttpxClient(transport=transport)\n",
    "\n",
    "# You can use any openai compatible endpoint here!\n",
    "llm_client = OpenAI(\n",
    "    api_key = '', \n",
    "    base_url=\"http://proxy.dlai.link/coursera_proxy/together/\",\n",
    "   http_client=http_client, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79c892d4-69af-462d-9c45-71979ff6cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is no need to trace as the auto_instrument was set to true\n",
    "def query_openai(prompt):\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant from a customer support.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68b77517-41b5-454b-b719-9666dbf387bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.chain\n",
    "def rag_pipeline(query):\n",
    "    # Execute the query\n",
    "    retrieved_documents = retrieve(query)\n",
    "    context = format_context(retrieved_documents)\n",
    "    \n",
    "    # Create a prompt with the retrieved information\n",
    "    final_prompt = create_prompt(query, context)\n",
    "    \n",
    "    # Execute the OpenAI query\n",
    "    final_answer = query_openai(final_prompt)\n",
    "\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5170232a-4d6b-49e2-8f89-5903aa7e2f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, the answer to your question is:\n",
      "\n",
      "\"No, you cannot get a refund or exchange for another shirt if it's a sale item, unless stated otherwise. However, if the item is not a sale item and you initiate an exchange through our Returns Center within 30 days of delivery, we can process your return and provide a replacement. Please note that return shipping costs are covered for domestic returns, but you will be responsible for the cost of international returns.\"\n"
     ]
    }
   ],
   "source": [
    "response = rag_pipeline(\"Can I get a refund or exchange for another shirt?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff39c3b4-4eb1-4759-998b-4a7332e4c75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, the answer to the FAQ question \"What are your working hours?\" is:\n",
      "\n",
      "Our customer service is available from 9:00 AM to 6:00 PM, Monday through Friday.\n"
     ]
    }
   ],
   "source": [
    "response = rag_pipeline(\"What are your working hours?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "039e85b5-dbd5-4ce1-8569-94334e071a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mFOLLOW THIS URL TO OPEN THE UI: http://widxvlspbcig.labs.coursera.org\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Checkout the traces in the Phoenix UI!\n",
    "utils.make_url()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ffd195-5569-46ca-bffe-69036c49e036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
